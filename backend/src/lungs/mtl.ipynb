{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def7e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96def93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Memory: 4.29 GB\n",
      "Batch size: 32, Num workers: 4\n"
     ]
    }
   ],
   "source": [
    "# Configuration with optimizations\n",
    "data_dir = r'C:\\Users\\Jatin\\Desktop\\oncogenesis\\data\\lungs_data'\n",
    "batch_size = 32  # Increased from 16 for better GPU utilization\n",
    "num_classes = 3  # Adenocarcinoma, Benign_Tissue, Squamous_Cell_Carcinoma\n",
    "epochs = 60\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Enable cuDNN autotuner for faster convolutions\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "print(f\"Batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "538ed322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Gaussian Noise augmentation\n",
    "class GaussianNoise:\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "\n",
    "# Training transforms with extensive augmentation for histopathology images\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Higher resolution for histopathology\n",
    "    transforms.RandomRotation(90),  # Histopathology images can be rotated in any direction\n",
    "    transforms.RandomResizedCrop((224, 224), scale=(0.6, 1.0)),  # Scale variation\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),  # Histopathology images can be flipped\n",
    "    transforms.ColorJitter(\n",
    "        brightness=0.3,\n",
    "        contrast=0.3,\n",
    "        saturation=0.2,\n",
    "        hue=0.1\n",
    "    ),\n",
    "    transforms.RandomAffine(\n",
    "        degrees=90,\n",
    "        translate=(0.15, 0.15),\n",
    "        scale=(0.8, 1.2),\n",
    "        shear=10\n",
    "    ),\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNet stats\n",
    "    GaussianNoise(0., 0.01)  # Slight noise for robustness\n",
    "])\n",
    "\n",
    "# Validation/Test transforms (no augmentation)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1d51a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 15000 total images\n",
      "Train: 12000, Validation: 1500, Test: 1500\n",
      "Classes: ['Adenocarcinoma', 'Benign_Tissue', 'Squamous_Cell_Carcinoma']\n",
      "Class distribution in training: [4010 4041 3949]\n"
     ]
    }
   ],
   "source": [
    "# Load the full dataset\n",
    "full_dataset = datasets.ImageFolder(root=data_dir, transform=train_transform)\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = int(0.1 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    full_dataset, [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Calculate class weights for handling class imbalance\n",
    "train_targets = [full_dataset.targets[i] for i in train_dataset.indices]\n",
    "class_sample_counts = np.bincount(train_targets)\n",
    "class_weights = 1. / class_sample_counts\n",
    "samples_weights = [class_weights[t] for t in train_targets]\n",
    "\n",
    "print(f\"Dataset loaded: {len(full_dataset)} total images\")\n",
    "print(f\"Train: {train_size}, Validation: {val_size}, Test: {test_size}\")\n",
    "print(f\"Classes: {full_dataset.classes}\")\n",
    "print(f\"Class distribution in training: {class_sample_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d0bc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaders created with optimizations:\n",
      "Train batches: 375, Val batches: 24, Test batches: 24\n",
      "Train batch size: 32, Val/Test batch size: 64\n"
     ]
    }
   ],
   "source": [
    "# Create weighted sampler for balanced training\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=samples_weights, \n",
    "    num_samples=len(samples_weights), \n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# Set validation and test transforms (no augmentation)\n",
    "val_dataset.dataset.transform = val_transform\n",
    "test_dataset.dataset.transform = val_transform\n",
    "\n",
    "# Create optimized data loaders (Windows-compatible)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    sampler=sampler,\n",
    "    num_workers=0,  # Set to 0 for Windows to avoid multiprocessing issues\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size * 2,  # Larger batch for validation (no gradients)\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size * 2, \n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Data loaders created with optimizations:\")\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}, Test batches: {len(test_loader)}\")\n",
    "print(f\"Train batch size: {batch_size}, Val/Test batch size: {batch_size * 2}\")\n",
    "print(\"Note: num_workers=0 for Windows compatibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a8865c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model initialized with 16,559,683 trainable parameters (Total: 16,559,683)\n"
     ]
    }
   ],
   "source": [
    "# Advanced CNN architecture for lung histopathology classification\n",
    "class LungCNN(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(LungCNN, self).__init__()\n",
    "        \n",
    "        # First convolution block - capture basic features\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # Second convolution block - intermediate features\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Third convolution block - complex features\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "        \n",
    "        # Fourth convolution block - high-level features\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        # Adaptive pooling to handle variable input sizes\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))\n",
    "        \n",
    "        # Fully connected layers with progressive dimension reduction\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 4 * 4, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights using Kaiming initialization\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = LungCNN(num_classes=num_classes).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nModel initialized with {trainable_params:,} trainable parameters (Total: {total_params:,})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25ce603a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "Optimizer: AdamW with weight decay 0.01\n",
      "Scheduler: OneCycleLR with max_lr=0.001\n",
      "Loss: CrossEntropyLoss with class weights\n",
      "Mixed Precision (AMP): Enabled\n",
      "Class weights: [0.00024938 0.00024746 0.00025323]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jatin\\AppData\\Local\\Temp\\ipykernel_19248\\717511980.py:38: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "# Mixup data augmentation for better generalization\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "# Loss function with class weights\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "# Optimizer with weight decay (L2 regularization)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduler with warmup\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=0.001,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    pct_start=0.2,  # Warmup for first 20% of training\n",
    "    div_factor=25,  # LR starts at max_lr/25\n",
    "    final_div_factor=1000  # Final LR is max_lr/1000\n",
    ")\n",
    "\n",
    "# Initialize gradient scaler for mixed precision training\n",
    "scaler = GradScaler()\n",
    "use_amp = torch.cuda.is_available()  # Use AMP only if CUDA is available\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"Optimizer: AdamW with weight decay 0.01\")\n",
    "print(f\"Scheduler: OneCycleLR with max_lr=0.001\")\n",
    "print(f\"Loss: CrossEntropyLoss with class weights\")\n",
    "print(f\"Mixed Precision (AMP): {'Enabled' if use_amp else 'Disabled'}\")\n",
    "print(f\"Class weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54896f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with optimizations...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/60 [Train]:   0%|          | 0/375 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Training function with progress tracking and optimizations\n",
    "def plot_confusion_matrix(y_true, y_pred, classes):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted', fontsize=12)\n",
    "    plt.ylabel('True', fontsize=12)\n",
    "    plt.title('Confusion Matrix - Lung Cancer Classification', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Training tracking\n",
    "best_val_acc = 0\n",
    "best_model_state = None\n",
    "patience = 12\n",
    "patience_counter = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "val_predictions = []\n",
    "val_targets = []\n",
    "\n",
    "print(\"Starting training with optimizations...\\n\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # ==================== Training Phase ====================\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    train_loop = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train]')\n",
    "    for batch_idx, (images, labels) in enumerate(train_loop):\n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        \n",
    "        # Apply mixup augmentation\n",
    "        images, labels_a, labels_b, lam = mixup_data(images, labels, alpha=0.2)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)  # Faster than zero_grad()\n",
    "        \n",
    "        # Mixed precision training\n",
    "        with autocast(enabled=use_amp):\n",
    "            outputs = model(images)\n",
    "            loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
    "        \n",
    "        # Backward pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Metrics\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (lam * (predicted == labels_a).sum().item()\n",
    "                   + (1 - lam) * (predicted == labels_b).sum().item())\n",
    "        \n",
    "        # Update progress bar less frequently for speed\n",
    "        if batch_idx % 5 == 0:\n",
    "            train_loop.set_postfix({\n",
    "                'Loss': f'{running_loss/total:.4f}',\n",
    "                'Acc': f'{correct/total:.4f}',\n",
    "                'LR': f'{optimizer.param_groups[0][\"lr\"]:.6f}'\n",
    "            })\n",
    "    \n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # ==================== Validation Phase ====================\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_preds = []\n",
    "    val_true = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=f'Epoch {epoch+1}/{epochs} [Val]'):\n",
    "            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            \n",
    "            with autocast(enabled=use_amp):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            val_preds.extend(predicted.cpu().numpy())\n",
    "            val_true.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_loss = val_loss / val_total\n",
    "    val_acc = val_correct / val_total\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    # Save predictions for confusion matrix\n",
    "    val_predictions = val_preds\n",
    "    val_targets = val_true\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch+1}/{epochs} Summary:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # ==================== Model Saving & Early Stopping ====================\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'class_names': full_dataset.classes,\n",
    "        }, 'lung_cnn_best.pth')\n",
    "        print(f\"✓ New best model saved! Validation Accuracy: {best_val_acc:.4f}\\n\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement. Patience: {patience_counter}/{patience}\\n\")\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs!\")\n",
    "            print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "            print(f\"{'='*60}\\n\")\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"\\n✓ Loaded best model with validation accuracy: {best_val_acc:.4f}\\n\")\n",
    "    \n",
    "    # Plot final confusion matrix\n",
    "    plot_confusion_matrix(val_targets, val_predictions, full_dataset.classes)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(val_targets, val_predictions, target_names=full_dataset.classes))\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ec4b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(train_losses, label='Train Loss', linewidth=2, color='#2E86AB')\n",
    "axes[0].plot(val_losses, label='Validation Loss', linewidth=2, color='#A23B72')\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(train_accs, label='Train Accuracy', linewidth=2, color='#2E86AB')\n",
    "axes[1].plot(val_accs, label='Validation Accuracy', linewidth=2, color='#A23B72')\n",
    "axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"Final Train Accuracy: {train_accs[-1]:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_accs[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4240ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set with optimizations\n",
    "print(\"Evaluating on test set...\\n\")\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "test_preds = []\n",
    "test_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc='Testing'):\n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        \n",
    "        with autocast(enabled=use_amp):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        test_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        test_preds.extend(predicted.cpu().numpy())\n",
    "        test_true.extend(labels.cpu().numpy())\n",
    "\n",
    "test_loss = test_loss / test_total\n",
    "test_acc = test_correct / test_total\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Test Set Results:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Plot test confusion matrix\n",
    "plot_confusion_matrix(test_true, test_preds, full_dataset.classes)\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"\\nTest Set Classification Report:\")\n",
    "print(classification_report(test_true, test_preds, target_names=full_dataset.classes, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e9bb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the complete model and checkpoint\n",
    "print(\"Saving models...\\n\")\n",
    "\n",
    "# Save the full model\n",
    "torch.save(model, \"lung_cnn_full_model.pth\")\n",
    "\n",
    "# Save a comprehensive checkpoint with all necessary information\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'class_names': full_dataset.classes,\n",
    "    'class_to_idx': full_dataset.class_to_idx,\n",
    "    'input_size': (224, 224),  # The image size we used for training\n",
    "    'normalize_mean': [0.485, 0.456, 0.406],  # ImageNet stats\n",
    "    'normalize_std': [0.229, 0.224, 0.225],\n",
    "    'best_val_acc': best_val_acc,\n",
    "    'test_acc': test_acc,\n",
    "    'num_classes': num_classes,\n",
    "    'training_history': {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'val_accs': val_accs\n",
    "    }\n",
    "}\n",
    "torch.save(checkpoint, \"lung_cnn_checkpoint.pth\")\n",
    "\n",
    "# Save class names separately for easy access\n",
    "with open(\"lung_class_names.pkl\", \"wb\") as f:\n",
    "    pickle.dump(full_dataset.classes, f)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Models saved successfully!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nSaved files:\")\n",
    "print(\"1. 'lung_cnn_full_model.pth' - Full model for simple loading\")\n",
    "print(\"2. 'lung_cnn_best.pth' - Best checkpoint during training\")\n",
    "print(\"3. 'lung_cnn_checkpoint.pth' - Complete checkpoint with all parameters\")\n",
    "print(\"4. 'lung_class_names.pkl' - Class names mapping\")\n",
    "print(\"\\nModel Summary:\")\n",
    "print(f\"Classes: {full_dataset.classes}\")\n",
    "print(f\"Class mapping: {full_dataset.class_to_idx}\")\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Input Size: 224x224\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3faa972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: How to load and use the model for inference\n",
    "print(\"Example: Loading model for inference\\n\")\n",
    "\n",
    "# Method 1: Load the full model\n",
    "loaded_model = torch.load(\"lung_cnn_full_model.pth\")\n",
    "loaded_model.eval()\n",
    "\n",
    "# Method 2: Load from checkpoint (more flexible)\n",
    "checkpoint_loaded = torch.load(\"lung_cnn_checkpoint.pth\")\n",
    "model_for_inference = LungCNN(num_classes=checkpoint_loaded['num_classes']).to(device)\n",
    "model_for_inference.load_state_dict(checkpoint_loaded['model_state_dict'])\n",
    "model_for_inference.eval()\n",
    "\n",
    "print(\"✓ Model loaded successfully!\")\n",
    "print(f\"Classes: {checkpoint_loaded['class_names']}\")\n",
    "print(f\"Model accuracy on test set: {checkpoint_loaded['test_acc']:.4f}\")\n",
    "\n",
    "# Load class names\n",
    "with open(\"lung_class_names.pkl\", \"rb\") as f:\n",
    "    class_names = pickle.load(f)\n",
    "print(f\"\\nClass names loaded: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe09122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample predictions\n",
    "print(\"Visualizing sample predictions from test set...\\n\")\n",
    "\n",
    "# Get a batch from test loader\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "    probabilities = F.softmax(outputs, dim=1)\n",
    "\n",
    "# Denormalize images for visualization\n",
    "def denormalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor\n",
    "\n",
    "# Plot sample predictions\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx in range(min(8, len(images))):\n",
    "    img = images[idx].cpu().clone()\n",
    "    img = denormalize(img)\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].axis('off')\n",
    "    \n",
    "    true_label = full_dataset.classes[labels[idx]]\n",
    "    pred_label = full_dataset.classes[predictions[idx]]\n",
    "    confidence = probabilities[idx][predictions[idx]].item() * 100\n",
    "    \n",
    "    color = 'green' if predictions[idx] == labels[idx] else 'red'\n",
    "    axes[idx].set_title(f'True: {true_label}\\nPred: {pred_label}\\nConf: {confidence:.1f}%',\n",
    "                       fontsize=10, color=color, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Sample Predictions from Test Set', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
